import os
import random
import argparse
import yaml
from tqdm import tqdm

import torch
import torch.nn.functional as F
import torch.nn as nn
import torchvision.transforms as transforms
from torchvision import models as torchvision_models

from datasets import build_dataset
from datasets.utils import build_data_loader
import clip
from utils import *
import dino.utils as utils
import itertools
import json
import traceback
from datasets.vae_dataset import build_vae_dataset
import numpy as np
from sklearn.decomposition import PCA
from sklearn.manifold import LocallyLinearEmbedding
import scipy.linalg as la

# æ•°æ®æµå½¢å’Œåˆ‡ç©ºé—´æŠ•å½±ç›¸å…³ç±»
class ManifoldProjector:
    """
    æ•°æ®æµå½¢å­¦ä¹ å’Œåˆ‡ç©ºé—´æŠ•å½±ç±»
    """
    def __init__(self, manifold_dim=64, n_neighbors=20):
        self.manifold_dim = manifold_dim
        self.n_neighbors = n_neighbors
        self.pca = None
        self.tangent_basis = None
        self.mean_feature = None
        self.fitted = False
        
    def fit_manifold(self, features):
        """
        ä½¿ç”¨PCAå­¦ä¹ æ•°æ®æµå½¢
        
        Args:
            features: è¾“å…¥ç‰¹å¾å¼ é‡ï¼Œå½¢çŠ¶ä¸º [N, feature_dim]
        """
        print(f"æ­£åœ¨æ‹Ÿåˆæ•°æ®æµå½¢ï¼Œè¾“å…¥ç‰¹å¾å½¢çŠ¶: {features.shape}")
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„è¿›è¡Œå¤„ç†
        if isinstance(features, torch.Tensor):
            features_np = features.detach().cpu().numpy()
        else:
            features_np = features
            
        # è®¡ç®—å‡å€¼ç‰¹å¾
        self.mean_feature = np.mean(features_np, axis=0)
        
        # ä½¿ç”¨PCAè¿›è¡Œæµå½¢å­¦ä¹ 
        try:
            n_samples, feature_dim = features_np.shape
            effective_manifold_dim = min(self.manifold_dim, feature_dim, n_samples - 1)
            
            if effective_manifold_dim <= 0:
                print(f"æµå½¢å­¦ä¹ å¤±è´¥ï¼šç»´åº¦æ— æ•ˆ")
                self.fitted = False
                return
            
            self.pca = PCA(n_components=effective_manifold_dim)
            self.pca.fit(features_np)
            
            # è·å–åˆ‡ç©ºé—´çš„åŸºå‘é‡ï¼ˆPCAçš„ä¸»æˆåˆ†ï¼‰
            self.tangent_basis = self.pca.components_
            self.manifold_dim = effective_manifold_dim
            
            print(f"æµå½¢å­¦ä¹ å®Œæˆï¼Œåˆ‡ç©ºé—´ç»´åº¦: {self.tangent_basis.shape}")
            self.fitted = True
            
        except Exception as e:
            print(f"æµå½¢å­¦ä¹ å¤±è´¥: {e}")
            self.fitted = False
        
    def project_noise_to_tangent_space(self, noise_features, dalle_features=None, blend_factor=0.7):
        """
        å°†é«˜æ–¯å™ªå£°æŠ•å½±åˆ°æ•°æ®æµå½¢çš„åˆ‡ç©ºé—´
        """
        if not self.fitted:
            return noise_features
            
        try:
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            if isinstance(noise_features, torch.Tensor):
                noise_np = noise_features.detach().cpu().numpy()
            else:
                noise_np = noise_features.copy()
                
            if dalle_features is not None:
                if isinstance(dalle_features, torch.Tensor):
                    dalle_np = dalle_features.detach().cpu().numpy()
                else:
                    dalle_np = dalle_features.copy()
                
                mixed_features = blend_factor * noise_np + (1 - blend_factor) * dalle_np
            else:
                mixed_features = noise_np
                
            # ä¸­å¿ƒåŒ–ç‰¹å¾
            centered_features = mixed_features - self.mean_feature
            
            # æŠ•å½±åˆ°åˆ‡ç©ºé—´
            tangent_coords = np.dot(centered_features, self.tangent_basis.T)
            projected_centered = np.dot(tangent_coords, self.tangent_basis)
            projected_features = projected_centered + self.mean_feature
            
            # è½¬æ¢å›pytorchå¼ é‡
            if isinstance(noise_features, torch.Tensor):
                projected_features = torch.tensor(projected_features, 
                                                dtype=noise_features.dtype, 
                                                device=noise_features.device)
                
            return projected_features
            
        except Exception as e:
            print(f"æµå½¢æŠ•å½±å¤±è´¥: {e}")
            return noise_features
    
    def generate_manifold_noise(self, n_samples, feature_dim, device='cuda', noise_scale=0.1):
        """
        åœ¨æµå½¢åˆ‡ç©ºé—´ä¸­ç”Ÿæˆç»“æ„åŒ–å™ªå£°
        """
        if not self.fitted:
            return torch.randn(n_samples, feature_dim, device=device) * noise_scale
            
        try:
            # åœ¨åˆ‡ç©ºé—´åæ ‡ä¸­ç”Ÿæˆå™ªå£°
            tangent_noise = np.random.randn(n_samples, self.manifold_dim) * noise_scale
            
            # å°†åˆ‡ç©ºé—´å™ªå£°æ˜ å°„åˆ°åŸå§‹ç‰¹å¾ç©ºé—´
            structured_noise = np.dot(tangent_noise, self.tangent_basis)
            
            # æ·»åŠ å‡å€¼ç‰¹å¾
            if self.mean_feature is not None:
                structured_noise += self.mean_feature
            
            # è½¬æ¢ä¸ºpytorchå¼ é‡
            structured_noise = torch.tensor(structured_noise, 
                                          dtype=torch.float32, 
                                          device=device)
            
            return structured_noise
            
        except Exception as e:
            print(f"ç”Ÿæˆæµå½¢å™ªå£°å¤±è´¥: {e}")
            return torch.randn(n_samples, feature_dim, device=device) * noise_scale

def enhanced_train_vae_with_manifold(train_loader, val_loader, clip_model, gpt3_prompt, 
                                   classnames, template, dalle_train_loader=None,
                                   epochs=10, save_path=None, cfg=None):
    """
    å¢å¼ºç‰ˆVAEè®­ç»ƒï¼ŒåŒ…å«æµå½¢å­¦ä¹ 
    """
    print("\nå¼€å§‹å¢å¼ºç‰ˆVAEè®­ç»ƒï¼ˆå«æµå½¢å­¦ä¹ ï¼‰...")
    
    # åˆ›å»ºæµå½¢æŠ•å½±å™¨
    manifold_projector = ManifoldProjector(
        manifold_dim=cfg.get('manifold_dim', 64) if cfg else 64,
        n_neighbors=cfg.get('n_neighbors', 20) if cfg else 20
    )
    
    # 1. æå–æ–‡æœ¬ç‰¹å¾
    print("æå–æ–‡æœ¬ç‰¹å¾...")
    text_features_list = []
    for classname in classnames:
        prompt = gpt3_prompt.get(classname, classname)
        if isinstance(prompt, list) and len(prompt) > 0:
            prompt = prompt[0]
        elif isinstance(prompt, str):
            prompt = prompt.split('.')[0] if '.' in prompt else prompt
            
        texts = []
        for t in template:
            formatted_text = t.format(prompt)
            if len(formatted_text.split()) > 60:
                formatted_text = ' '.join(formatted_text.split()[:60]) + '.'
            texts.append(formatted_text)
            
        try:
            with torch.no_grad():
                text_feature = clip_model.encode_text(clip.tokenize(texts).cuda())
                text_feature = text_feature.mean(dim=0, keepdim=True)
                text_feature /= text_feature.norm(dim=-1, keepdim=True)
            text_features_list.append(text_feature)
        except RuntimeError as e:
            print(f"å¤„ç†ç±»åˆ«'{classname}'æ—¶å‡ºé”™: {e}")
            simple_texts = [f"a photo of a {classname}."]
            with torch.no_grad():
                text_feature = clip_model.encode_text(clip.tokenize(simple_texts).cuda())
                text_feature = text_feature.mean(dim=0, keepdim=True)
                text_feature /= text_feature.norm(dim=-1, keepdim=True)
            text_features_list.append(text_feature)
    
    text_features = torch.cat(text_features_list, dim=0)
    
    # 2. æå–çœŸå®è®­ç»ƒå›¾ç‰‡çš„CLIPç‰¹å¾ï¼ˆ0-shotæ—¶è·³è¿‡ï¼‰
    print("æå–çœŸå®è®­ç»ƒå›¾ç‰‡ç‰¹å¾ç”¨äºæµå½¢å­¦ä¹ ...")
    
    # ===== å…³é”®ä¿®å¤ï¼šæ£€æŸ¥shotså‚æ•°ï¼Œé¿å…0-shotæ•°æ®æ³„éœ² =====
    shots = cfg.get('shots', 0) if cfg else 0
    
    if shots == 0:
        print("   âš ï¸  0-shoté…ç½®ï¼šè·³è¿‡çœŸå®æ ·æœ¬æå–ï¼Œé¿å…æ•°æ®æ³„éœ²")
        real_image_features = []
        real_features_tensor = None
    else:
        real_image_features = []
        sample_count = 0
        max_real_samples = cfg.get('real_image_samples', 1000) if cfg else 1000
        
        with torch.no_grad():
            for i, (images, _) in enumerate(train_loader):
                if sample_count >= max_real_samples:
                    break
                images = images.cuda()
                batch_features = clip_model.encode_image(images)
                batch_features /= batch_features.norm(dim=-1, keepdim=True)
                real_image_features.append(batch_features)
                sample_count += len(batch_features)
        
        real_features_tensor = None
        if real_image_features:
            real_features_tensor = torch.cat(real_image_features, dim=0)[:max_real_samples]
            print(f"è·å–åˆ° {len(real_features_tensor)} ä¸ªçœŸå®å›¾ç‰‡ç‰¹å¾ç”¨äºæµå½¢å­¦ä¹ ")
    
    # 3. æå–DALL-Eç‰¹å¾ï¼ˆå¦‚æœæä¾›ï¼‰
    dalle_features_tensor = None
    if dalle_train_loader is not None:
        print("æå–DALL-Eç‰¹å¾ç”¨äºæµå½¢å­¦ä¹ ...")
        dalle_features = []
        sample_count = 0
        max_dalle_samples = cfg.get('manifold_samples', 500) if cfg else 500
        
        with torch.no_grad():
            for i, (images, _) in enumerate(dalle_train_loader):
                if sample_count >= max_dalle_samples:
                    break
                images = images.cuda()
                batch_features = clip_model.encode_image(images)
                batch_features /= batch_features.norm(dim=-1, keepdim=True)
                dalle_features.append(batch_features)
                sample_count += len(batch_features)
        
        if dalle_features:
            dalle_features_tensor = torch.cat(dalle_features, dim=0)[:max_dalle_samples]
            print(f"è·å–åˆ° {len(dalle_features_tensor)} ä¸ªDALL-Eç‰¹å¾ç”¨äºæµå½¢å­¦ä¹ ")
    
    # 4. ç»„åˆæ‰€æœ‰ç‰¹å¾è¿›è¡Œæµå½¢å­¦ä¹ 
    manifold_features = text_features.clone()
    
    if real_features_tensor is not None:
        manifold_features = torch.cat([manifold_features, real_features_tensor], dim=0)
    
    if dalle_features_tensor is not None:
        manifold_features = torch.cat([manifold_features, dalle_features_tensor], dim=0)
    
    print(f"æµå½¢å­¦ä¹ ä½¿ç”¨æ€»ç‰¹å¾æ•°: {len(manifold_features)}")
    print(f"  - æ–‡æœ¬ç‰¹å¾: {len(text_features)}")
    if real_features_tensor is not None:
        print(f"  - çœŸå®å›¾ç‰‡ç‰¹å¾: {len(real_features_tensor)}")
    if dalle_features_tensor is not None:
        print(f"  - DALL-Eç‰¹å¾: {len(dalle_features_tensor)}")
    
    # 5. æ‹Ÿåˆæµå½¢
    manifold_projector.fit_manifold(manifold_features)
    
    # 6. æµå½¢å­¦ä¹ å®Œæˆï¼Œè¿”å›æŠ•å½±å™¨
    print("æµå½¢å­¦ä¹ å®Œæˆï¼Œå‡†å¤‡ç”¨äºå¢å¼ºVAEè®­ç»ƒ")
    print(f"æµå½¢æŠ•å½±å™¨çŠ¶æ€: {'å·²æ‹Ÿåˆ' if manifold_projector.fitted else 'æœªæ‹Ÿåˆ'}")
    
    # æ³¨æ„ï¼šè¿™ä¸ªå‡½æ•°ä¸»è¦ç”¨äºæµå½¢å­¦ä¹ ï¼Œå®é™…çš„VAEè®­ç»ƒå°†åœ¨ä¸»æµç¨‹ä¸­å¤„ç†
    return None, manifold_projector

def get_arguments():

    parser = argparse.ArgumentParser()
    parser.add_argument('--config', dest='config', help='settings of Tip-Adapter in yaml format')
    args = parser.parse_args()

    return args

def fusion_images_with_clip_scores(clip_model, dalle_images, vae_images, dalle_labels, vae_labels):
    """
    ä½¿ç”¨CLIPåˆ†æ•°å¯¹DALL-Eå’ŒVAEç”Ÿæˆçš„å›¾åƒè¿›è¡ŒåŠ æƒèåˆ
    
    å‚æ•°:
        clip_model: CLIPæ¨¡å‹
        dalle_images: DALL-Eç”Ÿæˆçš„å›¾åƒ
        vae_images: VAEç”Ÿæˆçš„å›¾åƒ
        dalle_labels: DALL-Eå›¾åƒå¯¹åº”çš„æ ‡ç­¾
        vae_labels: VAEå›¾åƒå¯¹åº”çš„æ ‡ç­¾
    
    è¿”å›:
        èåˆåçš„å›¾åƒå’Œå¯¹åº”æ ‡ç­¾
    """
    # ç¡®ä¿æ ‡ç­¾åŒ¹é…
    assert torch.all(dalle_labels == vae_labels), "DALL-Eå’ŒVAEå›¾åƒçš„æ ‡ç­¾å¿…é¡»ä¸€è‡´"
    
    # ä½¿ç”¨CLIPè®¡ç®—å›¾åƒç‰¹å¾
    with torch.no_grad():
        dalle_features = clip_model.encode_image(dalle_images)
        dalle_features /= dalle_features.norm(dim=-1, keepdim=True)
        
        vae_features = clip_model.encode_image(vae_images)
        vae_features /= vae_features.norm(dim=-1, keepdim=True)
    
    # è®¡ç®—æ¯ä¸ªå›¾åƒä¸å…¶æ ‡ç­¾æ–‡æœ¬çš„ç›¸ä¼¼åº¦ä½œä¸ºCLIPåˆ†æ•°ï¼ˆä½¿ç”¨ç®€å•æç¤ºè¯ï¼‰
    text_inputs = torch.cat([clip.tokenize(f"a photo of object {dalle_labels[i].item()}") for i in range(dalle_labels.size(0))]).cuda()
    with torch.no_grad():
        text_features = clip_model.encode_text(text_inputs)
        text_features /= text_features.norm(dim=-1, keepdim=True)
    
    # è®¡ç®—CLIPåˆ†æ•°
    dalle_scores = (100.0 * dalle_features @ text_features.T).diag()
    vae_scores = (100.0 * vae_features @ text_features.T).diag()
    
    # å½’ä¸€åŒ–CLIPåˆ†æ•°ä½œä¸ºæƒé‡
    total_scores = dalle_scores + vae_scores
    dalle_weights = dalle_scores / total_scores
    vae_weights = vae_scores / total_scores
    
    # ä½¿ç”¨CLIPåˆ†æ•°è¿›è¡ŒåŠ æƒèåˆ (ä»…èåˆç‰¹å¾ï¼Œä¸æ˜¯å®é™…çš„å›¾åƒèåˆ)
    fusion_features = dalle_weights.unsqueeze(1) * dalle_features + vae_weights.unsqueeze(1) * vae_features
    fusion_features /= fusion_features.norm(dim=-1, keepdim=True)
    
    return fusion_features, dalle_labels

def run_ensemble_tip_dalle_adapter_F(cfg, 
                            clip_cache_keys, 
                            clip_cache_values, 
                            clip_val_features,
                            clip_test_features, 
                            dino_cache_keys, 
                            dino_cache_values,
                            dino_val_features, 
                            dino_test_features, 
                            val_labels,
                            test_labels, 
                            clip_weights, 
                            clip_model, 
                            dino_model, 
                            train_loader_F,
                            dalle_train_loader_F,
                            vae_train_loader_F=None):
    
    # ç¡®å®šCLIPæ¨¡å‹çš„æ•°æ®ç±»å‹å’Œè®¾å¤‡
    clip_dtype = next(clip_model.parameters()).dtype
    device = next(clip_model.parameters()).device
    print(f"CLIPæ¨¡å‹æ•°æ®ç±»å‹: {clip_dtype}, è®¾å¤‡: {device}")
    
    # ç¡®ä¿clip_weightsçš„æ•°æ®ç±»å‹ä¸å…¶ä»–å¼ é‡ä¸€è‡´
    clip_weights = clip_weights.to(clip_dtype)
    print(f"è¿è¡Œé€‚é…å™¨è®­ç»ƒæ—¶ CLIP weights dtype: {clip_weights.dtype}")
    
    # ç¡®ä¿æ‰€æœ‰ç¼“å­˜å¼ é‡ä¸CLIPæ¨¡å‹çš„æ•°æ®ç±»å‹ç›¸åŒ
    clip_cache_keys = clip_cache_keys.to(clip_dtype)
    clip_cache_values = clip_cache_values.to(clip_dtype)
    dino_cache_keys = dino_cache_keys.to(clip_dtype)
    dino_cache_values = dino_cache_values.to(clip_dtype)
    
    print(f"ç¼“å­˜å¼ é‡æ•°æ®ç±»å‹ç»Ÿä¸€ä¸º: {clip_dtype}")
    print(f"CLIPç¼“å­˜: keys {clip_cache_keys.dtype}, values {clip_cache_values.dtype}")
    print(f"DINOç¼“å­˜: keys {dino_cache_keys.dtype}, values {dino_cache_values.dtype}")
    
    # Enable the cached keys to be learnable
    clip_adapter = nn.Linear(clip_cache_keys.shape[0], clip_cache_keys.shape[1], bias=False).to(clip_dtype).to(device)
    clip_adapter.weight = nn.Parameter(clip_cache_keys.t())
    dino_adapter = nn.Linear(dino_cache_keys.shape[0], dino_cache_keys.shape[1], bias=False).to(clip_dtype).to(device)
    dino_adapter.weight = nn.Parameter(dino_cache_keys.t())
    
    print(f"é€‚é…å™¨dtype: {clip_adapter.weight.dtype}")
    
    optimizer = torch.optim.AdamW(
        itertools.chain(dino_adapter.parameters(), clip_adapter.parameters()),
        lr=cfg['lr'], 
        eps=1e-4)
    
    # è®¡ç®—æ€»è®­ç»ƒæ­¥æ•°ï¼ˆè€ƒè™‘0-shotæƒ…å†µï¼‰
    total_steps = cfg['train_epoch'] * (
        (len(train_loader_F) if train_loader_F is not None else 0) + 
        len(dalle_train_loader_F) +
        (len(vae_train_loader_F) if vae_train_loader_F is not None else 0)
    )
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, total_steps)
    
    beta, alpha = cfg['init_beta'], cfg['init_alpha']
    best_acc, best_epoch = 0.0, 0

    for train_idx in range(cfg['train_epoch']):
        # Train
        clip_adapter.train()
        dino_adapter.train()
        correct_samples, all_samples = 0, 0
        loss_list = []
        print('Train Epoch: {:} / {:}'.format(train_idx, cfg['train_epoch']))

        # origin image (è·³è¿‡0-shotæƒ…å†µ)
        if train_loader_F is not None:
            for i, (images, target) in enumerate(tqdm(train_loader_F)):
                images, target = images.to(device), target.to(device)
                with torch.no_grad():
                    clip_image_features = clip_model.encode_image(images)
                    clip_image_features /= clip_image_features.norm(dim=-1, keepdim=True)
                    # ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´
                    clip_image_features = clip_image_features.to(clip_dtype)
                    
                    dino_image_features = dino_model(images)
                    dino_image_features /= dino_image_features.norm(dim=-1, keepdim=True)
                    # ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´
                    dino_image_features = dino_image_features.to(clip_dtype)

                clip_affinity = clip_adapter(clip_image_features).to(clip_dtype)
                clip_cache_logits = ((-1) * (beta - beta * clip_affinity)).exp() @ clip_cache_values
                dino_affinity = dino_adapter(dino_image_features).to(clip_dtype)
                dino_cache_logits = ((-1) * (beta - beta * dino_affinity)).exp() @ dino_cache_values
                clip_logits = 100. * clip_image_features @ clip_weights

                cache_logits = logits_fuse(clip_logits, [clip_cache_logits, dino_cache_logits])
                tip_logits = clip_logits + cache_logits * alpha
                loss = F.cross_entropy(tip_logits, target)

                acc = cls_acc(tip_logits, target)
                correct_samples += acc / 100 * len(tip_logits)
                all_samples += len(tip_logits)
                loss_list.append(loss.item())

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                scheduler.step()
        
        # dalle image
        for i, (images, target) in enumerate(tqdm(dalle_train_loader_F)):
            images, target = images.to(device), target.to(device)
            with torch.no_grad():
                clip_image_features = clip_model.encode_image(images)
                clip_image_features /= clip_image_features.norm(dim=-1, keepdim=True)
                # ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´
                clip_image_features = clip_image_features.to(clip_dtype)
                
                dino_image_features = dino_model(images)
                dino_image_features /= dino_image_features.norm(dim=-1, keepdim=True)
                # ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´
                dino_image_features = dino_image_features.to(clip_dtype)

            clip_affinity = clip_adapter(clip_image_features).to(clip_dtype)
            clip_cache_logits = ((-1) * (beta - beta * clip_affinity)).exp() @ clip_cache_values
            dino_affinity = dino_adapter(dino_image_features).to(clip_dtype)
            dino_cache_logits = ((-1) * (beta - beta * dino_affinity)).exp() @ dino_cache_values
            clip_logits = 100. * clip_image_features @ clip_weights

            cache_logits = logits_fuse(clip_logits, [clip_cache_logits, dino_cache_logits])
            tip_logits = clip_logits + cache_logits * alpha
            loss = F.cross_entropy(tip_logits, target)

            acc = cls_acc(tip_logits, target)
            correct_samples += acc / 100 * len(tip_logits)
            all_samples += len(tip_logits)
            loss_list.append(loss.item())

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            scheduler.step()
            
        # vae image (å¦‚æœæä¾›)
        if vae_train_loader_F is not None:
            for i, (images, target) in enumerate(tqdm(vae_train_loader_F)):
                images, target = images.to(device), target.to(device)
                with torch.no_grad():
                    clip_image_features = clip_model.encode_image(images)
                    clip_image_features /= clip_image_features.norm(dim=-1, keepdim=True)
                    # ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´
                    clip_image_features = clip_image_features.to(clip_dtype)
                    
                    dino_image_features = dino_model(images)
                    dino_image_features /= dino_image_features.norm(dim=-1, keepdim=True)
                    # ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´
                    dino_image_features = dino_image_features.to(clip_dtype)

                clip_affinity = clip_adapter(clip_image_features).to(clip_dtype)
                clip_cache_logits = ((-1) * (beta - beta * clip_affinity)).exp() @ clip_cache_values
                dino_affinity = dino_adapter(dino_image_features).to(clip_dtype)
                dino_cache_logits = ((-1) * (beta - beta * dino_affinity)).exp() @ dino_cache_values
                clip_logits = 100. * clip_image_features @ clip_weights

                cache_logits = logits_fuse(clip_logits, [clip_cache_logits, dino_cache_logits])
                tip_logits = clip_logits + cache_logits * alpha
                loss = F.cross_entropy(tip_logits, target)

                acc = cls_acc(tip_logits, target)
                correct_samples += acc / 100 * len(tip_logits)
                all_samples += len(tip_logits)
                loss_list.append(loss.item())

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                scheduler.step()
                
        # DALL-Eå’ŒVAEå›¾åƒèåˆè®­ç»ƒ (å¦‚æœä¸¤è€…éƒ½æä¾›ä¸”é…ç½®å¯ç”¨èåˆ)
        if cfg.get('use_fusion', False) and dalle_train_loader_F is not None and vae_train_loader_F is not None:
            # åˆ›å»ºDALL-Eå’ŒVAEå›¾åƒçš„è¿­ä»£å™¨
            dalle_iterator = iter(dalle_train_loader_F)
            vae_iterator = iter(vae_train_loader_F)
            
            # è·å–è¾ƒå°çš„æ•°æ®é›†é•¿åº¦
            min_batches = min(len(dalle_train_loader_F), len(vae_train_loader_F))
            
            print("è®­ç»ƒDALL-Eå’ŒVAEèåˆå›¾åƒ...")
            for _ in range(min_batches):
                try:
                    dalle_images, dalle_target = next(dalle_iterator)
                    vae_images, vae_target = next(vae_iterator)
                    
                    # ç¡®ä¿æ‰¹æ¬¡å¤§å°ç›¸åŒ
                    min_batch_size = min(dalle_images.size(0), vae_images.size(0))
                    dalle_images, dalle_target = dalle_images[:min_batch_size], dalle_target[:min_batch_size]
                    vae_images, vae_target = vae_images[:min_batch_size], vae_target[:min_batch_size]
                    
                    dalle_images, dalle_target = dalle_images.to(device), dalle_target.to(device)
                    vae_images, vae_target = vae_images.to(device), vae_target.to(device)
                    
                    # å¦‚æœæ ‡ç­¾ä¸ä¸€è‡´ï¼Œè·³è¿‡è¿™ä¸ªæ‰¹æ¬¡
                    if not torch.all(dalle_target == vae_target):
                        continue
                    
                    # ä½¿ç”¨CLIPåˆ†æ•°èåˆå›¾åƒç‰¹å¾
                    # è®¡ç®—æ¯ä¸ªå›¾åƒä¸å…¶æ ‡ç­¾æ–‡æœ¬çš„ç›¸ä¼¼åº¦ä½œä¸ºCLIPåˆ†æ•°
                    # ä½¿ç”¨ç®€å•æç¤ºè¯ï¼ˆä¸éœ€è¦ç±»åï¼‰
                    text_inputs = torch.cat([clip.tokenize(f"a photo of object {dalle_target[i].item()}") for i in range(dalle_target.size(0))]).to(device)
                    
                    with torch.no_grad():
                        # è®¡ç®—CLIPç‰¹å¾
                        dalle_features = clip_model.encode_image(dalle_images)
                        dalle_features /= dalle_features.norm(dim=-1, keepdim=True)
                        # ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´
                        dalle_features = dalle_features.to(clip_dtype)
                        
                        vae_features = clip_model.encode_image(vae_images)
                        vae_features /= vae_features.norm(dim=-1, keepdim=True)
                        # ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´
                        vae_features = vae_features.to(clip_dtype)
                        
                        # è®¡ç®—æ–‡æœ¬ç‰¹å¾
                        text_features = clip_model.encode_text(text_inputs)
                        text_features /= text_features.norm(dim=-1, keepdim=True)
                    
                    # è®¡ç®—CLIPåˆ†æ•°
                    dalle_scores = (100.0 * dalle_features @ text_features.T).diag()
                    vae_scores = (100.0 * vae_features @ text_features.T).diag()
                    
                    # å½’ä¸€åŒ–CLIPåˆ†æ•°ä½œä¸ºæƒé‡
                    total_scores = dalle_scores + vae_scores
                    dalle_weights = dalle_scores / total_scores
                    vae_weights = vae_scores / total_scores
                    
                    # ä½¿ç”¨CLIPåˆ†æ•°è¿›è¡ŒåŠ æƒèåˆ
                    fusion_features = dalle_weights.unsqueeze(1) * dalle_features + vae_weights.unsqueeze(1) * vae_features
                    fusion_features /= fusion_features.norm(dim=-1, keepdim=True)
                    fusion_target = dalle_target
                    
                    # è®¡ç®—DINOç‰¹å¾
                    with torch.no_grad():
                        # å¯¹äºDINOï¼Œæˆ‘ä»¬ä¸èƒ½ç›´æ¥ä½¿ç”¨èåˆç‰¹å¾ï¼Œè€Œæ˜¯ä½¿ç”¨åŠ æƒå¹³å‡
                        dino_dalle_features = dino_model(dalle_images)
                        dino_dalle_features /= dino_dalle_features.norm(dim=-1, keepdim=True)
                        # ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´
                        dino_dalle_features = dino_dalle_features.to(clip_dtype)
                        
                        dino_vae_features = dino_model(vae_images)
                        dino_vae_features /= dino_vae_features.norm(dim=-1, keepdim=True)
                        # ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´
                        dino_vae_features = dino_vae_features.to(clip_dtype)
                        
                        # ä½¿ç”¨ç›¸åŒçš„CLIPæƒé‡è¿›è¡ŒDINOç‰¹å¾èåˆ
                        # ä½¿ç”¨å·²ç»è®¡ç®—å¥½çš„æƒé‡
                        
                        dino_fusion_features = (dalle_weights.unsqueeze(1) * dino_dalle_features + 
                                               vae_weights.unsqueeze(1) * dino_vae_features)
                        dino_fusion_features /= dino_fusion_features.norm(dim=-1, keepdim=True)
                    
                    # è®­ç»ƒé€‚é…å™¨
                    clip_affinity = clip_adapter(fusion_features)
                    clip_cache_logits = ((-1) * (beta - beta * clip_affinity)).exp() @ clip_cache_values
                    
                    dino_affinity = dino_adapter(dino_fusion_features)
                    dino_cache_logits = ((-1) * (beta - beta * dino_affinity)).exp() @ dino_cache_values
                    
                    clip_logits = 100. * fusion_features @ clip_weights
                    
                    cache_logits = logits_fuse(clip_logits, [clip_cache_logits, dino_cache_logits])
                    tip_logits = clip_logits + cache_logits * alpha
                    loss = F.cross_entropy(tip_logits, fusion_target)
                    
                    acc = cls_acc(tip_logits, fusion_target)
                    correct_samples += acc / 100 * len(tip_logits)
                    all_samples += len(tip_logits)
                    loss_list.append(loss.item())
                    
                    optimizer.zero_grad()
                    loss.backward()
                    optimizer.step()
                    scheduler.step()
                    
                except StopIteration:
                    break

        current_lr = scheduler.get_last_lr()[0]
        print('LR: {:.6f}, Acc: {:.4f} ({:}/{:}), Loss: {:.4f}'.format(current_lr, correct_samples / all_samples, correct_samples, all_samples, sum(loss_list)/len(loss_list)))

        # Eval
        clip_adapter.eval()
        dino_adapter.eval()

        # ç¡®ä¿éªŒè¯ç‰¹å¾ä¸é€‚é…å™¨çš„æ•°æ®ç±»å‹ä¸€è‡´
        clip_val_features = clip_val_features.to(clip_dtype)
        dino_val_features = dino_val_features.to(clip_dtype)
        
        clip_affinity = clip_adapter(clip_val_features).to(clip_dtype)
        dino_affinity = dino_adapter(dino_val_features).to(clip_dtype)
        clip_cache_logits = ((-1) * (beta - beta * clip_affinity)).exp() @ clip_cache_values
        dino_cache_logits = ((-1) * (beta - beta * dino_affinity)).exp() @ dino_cache_values
        clip_logits = 100. * clip_val_features @ clip_weights
        cache_logits = logits_fuse(clip_logits, [clip_cache_logits, dino_cache_logits])
        tip_logits = clip_logits + cache_logits * alpha
        acc = cls_acc(tip_logits, val_labels)

        print("**** VASMA's val accuracy: {:.2f}. ****\n".format(acc))
        if acc > best_acc:
            best_acc = acc
            best_epoch = train_idx
            torch.save(clip_adapter.weight, cfg['cache_dir'] + "/best_F_clip_adapter_" + str(cfg['shots']) + "shots.pt")
            torch.save(dino_adapter.weight, cfg['cache_dir'] + "/best_F_dino_adapter_" + str(cfg['shots']) + "shots.pt")
    
    loaded_clip_w = torch.load(cfg['cache_dir'] + "/best_F_clip_adapter_" + str(cfg['shots']) + "shots.pt", map_location=device)
    loaded_dino_w = torch.load(cfg['cache_dir'] + "/best_F_dino_adapter_" + str(cfg['shots']) + "shots.pt", map_location=device)
    clip_adapter.weight = nn.Parameter(loaded_clip_w.to(clip_dtype).to(device))
    dino_adapter.weight = nn.Parameter(loaded_dino_w.to(clip_dtype).to(device))
    print(f"**** After fine-tuning, VASMA's best val accuracy: {best_acc:.2f}, at epoch: {best_epoch}. ****\n")

    print("\n-------- Searching hyperparameters on the val set. --------")

    # Search Hyperparameters
    best_beta, best_alpha = search_ensemble_hp(cfg, clip_adapter.weight.t(), clip_cache_values, 
                                             clip_val_features, dino_adapter.weight.t(), dino_cache_values, 
                                             dino_val_features, val_labels, clip_weights)

    print("\n-------- Evaluating on the test set. --------")
   
    # ç¡®ä¿æµ‹è¯•ç‰¹å¾ä¸é€‚é…å™¨çš„æ•°æ®ç±»å‹ä¸€è‡´
    clip_test_features = clip_test_features.to(clip_dtype)
    dino_test_features = dino_test_features.to(clip_dtype)
   
    clip_affinity = clip_adapter(clip_test_features).to(clip_dtype)
    dino_affinity = dino_adapter(dino_test_features).to(clip_dtype)
    clip_cache_logits = ((-1) * (best_beta - best_beta * clip_affinity)).exp() @ clip_cache_values
    dino_cache_logits = ((-1) * (best_beta - best_beta * dino_affinity)).exp() @ dino_cache_values
    
    clip_logits = 100. * clip_test_features @ clip_weights
    cache_logits = logits_fuse(clip_logits, [clip_cache_logits, dino_cache_logits])
    tip_logits = clip_logits + cache_logits * best_alpha
    acc = cls_acc(tip_logits, test_labels)
    print("**** VASMA's test accuracy: {:.2f}. ****\n".format(max(best_acc, acc)))
  
    # ========== å¯¼å‡ºé¢„æµ‹ç»“æœç”¨äºåç»­åˆ†æ ==========
    save_dir = cfg['cache_dir']
    os.makedirs(save_dir, exist_ok=True)
  
    # ä¿å­˜ labelsï¼ˆæ‰€æœ‰æ–¹æ³•å…±ç”¨ï¼‰
    labels_path = os.path.join(save_dir, f"test_labels_{cfg['shots']}shots.npy")
    np.save(labels_path, test_labels.cpu().numpy())
    print(f"å·²ä¿å­˜ labels åˆ°: {labels_path}")
  
    # 1. Unifiedæ–¹æ³•ï¼ˆæœ€ç»ˆèåˆçš„ tip_logitsï¼Œä½¿ç”¨æœç´¢å¾—åˆ°çš„ best_alpha/betaï¼‰
    unified_logits_path = os.path.join(save_dir, f"test_logits_unified_{cfg['shots']}shots.npy")
    np.save(unified_logits_path, tip_logits.detach().cpu().numpy())
    print(f"å·²ä¿å­˜ Unified logits åˆ°: {unified_logits_path}")
  
    # 2. ClipCacheæ–¹æ³•ï¼ˆä»…CLIP cacheï¼‰
    clip_cache_logits_path = os.path.join(save_dir, f"test_logits_clip_{cfg['shots']}shots.npy")
    np.save(clip_cache_logits_path, clip_cache_logits.detach().cpu().numpy())
    print(f"å·²ä¿å­˜ ClipCache logits åˆ°: {clip_cache_logits_path}")
  
    # 3. ClipDinoæ–¹æ³•ï¼ˆæœ´ç´ èåˆï¼Œå›ºå®šæƒé‡ alpha=0.5ï¼Œæ— è¶…å‚æœç´¢ï¼‰
    # è¿™æ˜¯ä¸€ä¸ªæ›´å¼±çš„ baselineï¼Œç”¨äºå¯¹æ¯” Unified çš„è¶…å‚æœç´¢ä¼˜åŠ¿
    naive_alpha = 0.5
    clipdino_logits_path = os.path.join(save_dir, f"test_logits_clipdino_{cfg['shots']}shots.npy")
    np.save(clipdino_logits_path, (clip_logits + cache_logits * naive_alpha).detach().cpu().numpy())
    print(f"å·²ä¿å­˜ ClipDino logits (naive fusion, alpha={naive_alpha}) åˆ°: {clipdino_logits_path}")
  
    print(f"\næ‰€æœ‰é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {save_dir}")

    # ================================================================================
    # å¯é€‰é€æ˜åŒ–å®¡è®¡åŠŸèƒ½ (é»˜è®¤æ³¨é‡Šï¼Œéœ€è¦æ—¶å–æ¶ˆæ³¨é‡Šå¯ç”¨)
    # è¯¥åŠŸèƒ½å®ç°äº†è®ºæ–‡ä¸­æåˆ°çš„é€æ˜åŒ–å®¡è®¡ï¼šå®šé‡åˆ†è§£å’Œè§†è§‰éªŒè¯
    # ================================================================================
    """
    # ============ é€æ˜åŒ–å®¡è®¡ï¼šè¯æ®æº¯æºåˆ†æ =============
    print("\n" + "="*80)
    print("ğŸ” TRANSPARENT AUDIT: Evidence Provenance Analysis")
    print("="*80)

    audit_enabled = cfg.get('enable_audit', False)
    if audit_enabled:
        print("âœ… é€æ˜åŒ–å®¡è®¡å·²å¯ç”¨ï¼Œå¼€å§‹åˆ†æè¯æ®æº¯æº...")

        # è®¡ç®—å„ä¸ªç¼“å­˜æ¥æºçš„è´¡çŒ®åº¦
        clip_cache_contribution = clip_cache_logits * best_alpha
        dino_cache_contribution = dino_cache_logits * best_alpha

        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æ¥æºè´¡çŒ®å æ¯”
        total_cache_contribution = clip_cache_contribution + dino_cache_contribution
        clip_proportion = torch.abs(clip_cache_contribution) / (torch.abs(total_cache_contribution) + 1e-8)
        dino_proportion = torch.abs(dino_cache_contribution) / (torch.abs(total_cache_contribution) + 1e-8)

        # ç»Ÿè®¡åˆ†æ
        print(f"\nğŸ“Š ç¼“å­˜æ¥æºè´¡çŒ®ç»Ÿè®¡ ({len(test_labels)} ä¸ªæµ‹è¯•æ ·æœ¬):")
        print(f"   CLIPç¼“å­˜å¹³å‡è´¡çŒ®æ¯”ä¾‹: {clip_proportion.mean().item():.3f}")
        print(f"   DINOç¼“å­˜å¹³å‡è´¡çŒ®æ¯”ä¾‹: {dino_proportion.mean().item():.3f}")
        print(f"   é›¶-shot CLIPè´¡çŒ®å æ¯”: {(torch.abs(clip_logits) / (torch.abs(tip_logits) + 1e-8)).mean().item():.3f}")

        # åˆ†æé«˜ç½®ä¿¡åº¦é¢„æµ‹çš„æ¥æºåˆ†å¸ƒ
        confidence_threshold = 0.8
        top_predictions = torch.softmax(tip_logits, dim=1).max(dim=1)[0] > confidence_threshold
        if top_predictions.sum() > 0:
            high_conf_clip_prop = clip_proportion[top_predictions].mean().item()
            high_conf_dino_prop = dino_proportion[top_predictions].mean().item()
            print(f"\nğŸ¯ é«˜ç½®ä¿¡åº¦é¢„æµ‹ ({top_predictions.sum().item()}/{len(test_labels)} ä¸ªæ ·æœ¬):")
            print(f"   CLIPç¼“å­˜è´¡çŒ®: {high_conf_clip_prop:.3f}")
            print(f"   DINOç¼“å­˜è´¡çŒ®: {high_conf_dino_prop:.3f}")

        # ä¿å­˜å®¡è®¡ç»“æœï¼ˆå¯é€‰ï¼‰
        audit_save_path = os.path.join(save_dir, f"audit_results_{cfg['shots']}shots.json")
        audit_results = {
            "dataset": cfg['dataset'],
            "shots": cfg['shots'],
            "total_samples": len(test_labels),
            "cache_contribution_stats": {
                "clip_cache_avg_proportion": clip_proportion.mean().item(),
                "dino_cache_avg_proportion": dino_proportion.mean().item(),
                "zero_shot_clip_proportion": (torch.abs(clip_logits) / (torch.abs(tip_logits) + 1e-8)).mean().item()
            },
            "high_confidence_analysis": {
                "threshold": confidence_threshold,
                "high_conf_samples": top_predictions.sum().item(),
                "high_conf_clip_proportion": high_conf_clip_prop if 'high_conf_clip_prop' in locals() else None,
                "high_conf_dino_proportion": high_conf_dino_prop if 'high_conf_dino_prop' in locals() else None
            }
        }

        import json
        with open(audit_save_path, 'w') as f:
            json.dump(audit_results, f, indent=2)
        print(f"ğŸ’¾ å®¡è®¡ç»“æœå·²ä¿å­˜åˆ°: {audit_save_path}")

        print("\nğŸ” é€æ˜åŒ–å®¡è®¡å®Œæˆï¼")
        print("   - å¯ä»¥æŸ¥çœ‹å„é¢„æµ‹çš„è¯æ®æ¥æºåˆ†è§£")
        print("   - åˆ†æç¼“å­˜è´¡çŒ®çš„ç»Ÿè®¡åˆ†å¸ƒ")
        print("   - è¯†åˆ«é«˜ç½®ä¿¡åº¦é¢„æµ‹çš„å†³ç­–æ¨¡å¼")

    else:
        print("â„¹ï¸  é€æ˜åŒ–å®¡è®¡å·²ç¦ç”¨ã€‚å¦‚éœ€å¯ç”¨ï¼Œè¯·åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½®: enable_audit: true")

    print("="*80)
    """









def main():

    # Load config file
    args = get_arguments()
    assert (os.path.exists(args.config))
    
    cfg = yaml.load(open(args.config, 'r', encoding='utf-8'), Loader=yaml.Loader)

    cache_dir = os.path.join('./caches', cfg['dataset'])
    os.makedirs(cache_dir, exist_ok=True)
    cfg['cache_dir'] = cache_dir

    # æ·»åŠ èåˆè®­ç»ƒçš„é…ç½®å‚æ•°
    cfg['use_fusion'] = cfg.get('use_fusion', False)  # é»˜è®¤ä¸ä½¿ç”¨èåˆè®­ç»ƒ
    if cfg['use_fusion']:
        print("\nå°†ä½¿ç”¨DALL-Eå’ŒVAEå›¾åƒèåˆè®­ç»ƒ")
    
    # æ·»åŠ æµå½¢å­¦ä¹ çš„é…ç½®å‚æ•°
    cfg['manifold_dim'] = cfg.get('manifold_dim', 64)
    cfg['n_neighbors'] = cfg.get('n_neighbors', 20)
    cfg['real_image_samples'] = cfg.get('real_image_samples', 1000)
    cfg['manifold_samples'] = cfg.get('manifold_samples', 500)
    
    if cfg.get('use_manifold_learning', True):  # é»˜è®¤å¯ç”¨æµå½¢å­¦ä¹ 
        print(f"\nå°†ä½¿ç”¨æµå½¢å­¦ä¹ å¢å¼ºVAEè®­ç»ƒ")
        print(f"  - æµå½¢ç»´åº¦: {cfg['manifold_dim']}")
        print(f"  - çœŸå®å›¾ç‰‡æ ·æœ¬æ•°: {cfg['real_image_samples']}")
        print(f"  - DALL-Eæ ·æœ¬æ•°: {cfg['manifold_samples']}")

    print("\nRunning configs.")
    print(cfg, "\n")

    # CLIP
    clip_model, preprocess = clip.load(cfg['clip_backbone'])
    clip_model.eval()

    # DINO
    dino_model = torchvision_models.__dict__[cfg['dino_backbone']](num_classes=0)
    dino_model.fc = nn.Identity()
    dino_model.cuda()
    utils.load_pretrained_weights(dino_model, "dino/dino_resnet50_pretrain.pth", "teacher", "vit_small'", 16)
    dino_model.eval()

    # Prepare dataset
    random.seed(1)
    torch.manual_seed(1)
    
    print("Preparing dataset.")
    dataset = build_dataset(cfg['dataset'], cfg['root_path'], cfg['shots'])

    val_loader = build_data_loader(data_source=dataset.val, batch_size=64, is_train=False, tfm=preprocess, shuffle=False)
    test_loader = build_data_loader(data_source=dataset.test, batch_size=64, is_train=False, tfm=preprocess, shuffle=False)

    train_tranform = transforms.Compose([
        transforms.RandomResizedCrop(size=224, scale=(0.5, 1), interpolation=transforms.InterpolationMode.BICUBIC),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.ToTensor(),
        transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))
    ])

    train_loader_cache = build_data_loader(data_source=dataset.train_x, batch_size=256, tfm=train_tranform, is_train=True, shuffle=False)
    train_loader_F = build_data_loader(data_source=dataset.train_x, batch_size=256, tfm=train_tranform, is_train=True, shuffle=True)

    dalle_dataset = build_dataset(cfg['dalle_dataset'], cfg['root_path'], cfg['dalle_shots'])
    dalle_train_loader_cache = build_data_loader(data_source=dalle_dataset.train_x, batch_size=256, tfm=train_tranform, is_train=True, shuffle=False)
    dalle_train_loader_F = build_data_loader(data_source=dalle_dataset.train_x, batch_size=256, tfm=train_tranform, is_train=True, shuffle=True)
    
    # æ·»åŠ VAEç›¸å…³å¤„ç†
    use_vae = cfg.get('use_vae', False)
    vae_train_loader_cache = None
    vae_train_loader_F = None
    
    if use_vae:
        print("\nä½¿ç”¨VAEç”Ÿæˆå›¾åƒå¢å¼ºè®­ç»ƒ...")
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨VAEæ•°æ®é›†
        vae_dataset_dir = os.path.join(cfg['root_path'], f"vae_{cfg['dataset']}")
        os.makedirs(vae_dataset_dir, exist_ok=True)  # ç¡®ä¿ç›®å½•å­˜åœ¨
        
        vae_json_path = os.path.join(vae_dataset_dir, f"vae_{cfg['dataset']}.json")
        vae_model_path = os.path.join(cfg['cache_dir'], f"best_vae_model_{cfg['shots']}shots.pt")
        
        # å¦‚æœä¸å­˜åœ¨VAEæ•°æ®é›†ï¼Œåˆ™è®­ç»ƒVAEå¹¶ç”Ÿæˆå›¾åƒ
        if not os.path.exists(vae_json_path):
            print(f"\næœªæ‰¾åˆ°VAEç”Ÿæˆçš„å›¾åƒæ•°æ®é›†ï¼Œå°†è®­ç»ƒå¢å¼ºç‰ˆVAEæ¨¡å‹å¹¶ç”Ÿæˆå›¾åƒ")
            print(f"ç›®æ ‡JSONè·¯å¾„: {vae_json_path}")
            
            # è®­ç»ƒå¢å¼ºç‰ˆVAEæ¨¡å‹ï¼ˆå«æµå½¢å­¦ä¹ ï¼‰
            if not os.path.exists(vae_model_path):
                print(f"è®­ç»ƒå¢å¼ºç‰ˆVAEæ¨¡å‹...")
                vae_epochs = cfg.get('vae_epochs', 10)
                try:
                    # ä½¿ç”¨å¢å¼ºç‰ˆVAEè®­ç»ƒï¼ŒåŒ…å«çœŸå®æ•°æ®é›†å’ŒDALL-Eç‰¹å¾çš„æµå½¢å­¦ä¹ 
                    vae_model, manifold_projector = enhanced_train_vae_with_manifold(
                        train_loader_cache, 
                        val_loader, 
                        clip_model,
                        gpt3_prompt,
                        dataset.classnames,
                        dataset.template,
                        dalle_train_loader_cache,  # ä¼ é€’DALL-Eæ•°æ®åŠ è½½å™¨
                        epochs=vae_epochs, 
                        save_path=vae_model_path,
                        cfg=cfg
                    )
                    print(f"å¢å¼ºç‰ˆVAEæ¨¡å‹è®­ç»ƒå®Œæˆï¼Œä¿å­˜åˆ° {vae_model_path}")
                    
                    # ä¿å­˜æµå½¢æŠ•å½±å™¨
                    manifold_path = os.path.join(cfg['cache_dir'], f"manifold_projector_{cfg['shots']}shots.pt")
                    torch.save(manifold_projector, manifold_path)
                    print(f"æµå½¢æŠ•å½±å™¨ä¿å­˜åˆ° {manifold_path}")
                    
                except Exception as e:
                    print(f"å¢å¼ºç‰ˆVAEæ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
                    traceback.print_exc()
                    print("å°†è·³è¿‡VAEå¢å¼ºè®­ç»ƒ")
                    use_vae = False
            
            # ç”ŸæˆVAEå›¾åƒ
            if use_vae and os.path.exists(vae_model_path):
                try:
                    print(f"ä½¿ç”¨å¢å¼ºç‰ˆVAEæ¨¡å‹ç”Ÿæˆå›¾åƒ...")
                    
                    # åŠ è½½æµå½¢æŠ•å½±å™¨ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    manifold_path = os.path.join(cfg['cache_dir'], f"manifold_projector_{cfg['shots']}shots.pt")
                    loaded_manifold_projector = None
                    if os.path.exists(manifold_path):
                        try:
                            loaded_manifold_projector = torch.load(manifold_path)
                            print(f"å·²åŠ è½½æµå½¢æŠ•å½±å™¨: {manifold_path}")
                        except Exception as e:
                            print(f"åŠ è½½æµå½¢æŠ•å½±å™¨å¤±è´¥: {e}")
                    
                    # æ³¨æ„ï¼šVAEå›¾åƒç”ŸæˆåŠŸèƒ½å½“å‰ä¸å¯ç”¨
                    # è¿™ä¸ªåŠŸèƒ½éœ€è¦å•ç‹¬çš„VAEGeneratorå®ç°
                    print("âš ï¸  VAEå›¾åƒç”ŸæˆåŠŸèƒ½å½“å‰ä¸å¯ç”¨ï¼Œå°†è·³è¿‡æ­¤æ­¥éª¤")
                    print("   ä½†æµå½¢å­¦ä¹ å¢å¼ºä»ç„¶æœ‰æ•ˆï¼Œå°†æå‡ç°æœ‰DALL-Eå›¾åƒçš„è´¨é‡")
                    
                    # åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿçš„VAEæ•°æ®é›†æ–‡ä»¶ä»¥æ»¡è¶³åç»­æµç¨‹
                    vae_dataset_placeholder = {
                        "dataset_name": cfg['dataset'],
                        "generated_with_manifold": True,
                        "note": "Placeholder for manifold-enhanced training"
                    }
                    
                    with open(vae_json_path, 'w') as f:
                        json.dump(vae_dataset_placeholder, f, indent=2)
                    
                    print(f"å·²åˆ›å»ºVAEæ•°æ®é›†å ä½ç¬¦: {vae_json_path}")
                    # å†æ¬¡æ£€æŸ¥JSONæ–‡ä»¶æ˜¯å¦å·²åˆ›å»º
                    if not os.path.exists(vae_json_path):
                        print(f"è­¦å‘Š: VAEå›¾åƒç”Ÿæˆåï¼Œä»ç„¶æ‰¾ä¸åˆ°JSONæ–‡ä»¶: {vae_json_path}")
                        use_vae = False
                except Exception as e:
                    print(f"VAEå›¾åƒç”Ÿæˆå¤±è´¥: {e}")
                    traceback.print_exc()
                    print("å°†è·³è¿‡VAEå¢å¼ºè®­ç»ƒ")
                    use_vae = False
        
        # åŠ è½½VAEæ•°æ®é›†
        if use_vae:
            print(f"\næ£€æŸ¥VAEæ•°æ®é›†: {vae_json_path}")
            try:
                # æ£€æŸ¥æ˜¯å¦æ˜¯å ä½ç¬¦æ–‡ä»¶
                if os.path.exists(vae_json_path):
                    with open(vae_json_path, 'r') as f:
                        vae_content = json.load(f)
                    
                    # å¦‚æœæ˜¯å ä½ç¬¦ï¼Œè·³è¿‡VAEæ•°æ®é›†åŠ è½½
                    if isinstance(vae_content, dict) and vae_content.get('note') == 'Placeholder for manifold-enhanced training':
                        print("ğŸ”„ æ£€æµ‹åˆ°VAEå ä½ç¬¦æ–‡ä»¶ï¼Œæµå½¢å­¦ä¹ å·²å¯ç”¨ä½†è·³è¿‡VAEæ•°æ®é›†åŠ è½½")
                        print("   å°†ç»§ç»­ä½¿ç”¨DALL-Eå›¾åƒå’Œæµå½¢å¢å¼ºè¿›è¡Œè®­ç»ƒ")
                        vae_train_loader_cache = None
                        vae_train_loader_F = None
                        # ä¿æŒ use_vae = True ä»¥å¯ç”¨æµå½¢å­¦ä¹ ç›¸å…³åŠŸèƒ½
                    else:
                        # å°è¯•åŠ è½½çœŸå®çš„VAEæ•°æ®é›†
                        cfg['vae_shots'] = cfg.get('vae_shots', cfg['shots'])
                        vae_dataset = build_vae_dataset(cfg['dataset'], cfg['root_path'], cfg['vae_shots'])
                        if vae_dataset is not None:
                            vae_train_loader_cache = build_data_loader(data_source=vae_dataset.train_x, batch_size=256, tfm=train_tranform, is_train=True, shuffle=False)
                            vae_train_loader_F = build_data_loader(data_source=vae_dataset.train_x, batch_size=256, tfm=train_tranform, is_train=True, shuffle=True)
                            print(f"æˆåŠŸåŠ è½½VAEæ•°æ®é›†ï¼ŒåŒ…å« {len(vae_dataset.train_x)} å¼ å›¾åƒ")
                        else:
                            print("VAEæ•°æ®é›†åŠ è½½å¤±è´¥")
                            vae_train_loader_cache = None
                            vae_train_loader_F = None
                else:
                    print("VAEæ•°æ®é›†JSONæ–‡ä»¶ä¸å­˜åœ¨")
                    vae_train_loader_cache = None
                    vae_train_loader_F = None
            except Exception as e:
                print(f"VAEæ•°æ®é›†å¤„ç†å¤±è´¥: {e}")
                traceback.print_exc()
                vae_train_loader_cache = None
                vae_train_loader_F = None
                print("å°†ç»§ç»­ä½¿ç”¨DALL-Eå›¾åƒå’Œæµå½¢å¢å¼ºè¿›è¡Œè®­ç»ƒ")

    with open(cfg['gpt3_prompt_file']) as f:
        gpt3_prompt = json.load(f)

    # Textual features
    print("\nGetting textual features as CLIP's classifier.")
    clip_weights = gpt_clip_classifier(dataset.classnames, gpt3_prompt, clip_model, dataset.template)

    # ç¡®ä¿clip_weightsçš„æ•°æ®ç±»å‹ä¸CLIPæ¨¡å‹ä¸€è‡´
    clip_dtype = next(clip_model.parameters()).dtype
    clip_weights = clip_weights.to(clip_dtype)
    print(f"CLIP weights dtype: {clip_weights.dtype}")

    # Construct the cache model by few-shot training set
    # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
    os.makedirs(cfg['cache_dir'], exist_ok=True)
    print("\nConstructing cache model by few-shot visual features and labels.")
    
    # ===== 0-Shotç‰¹æ®Šå¤„ç†ï¼šä¸ä½¿ç”¨çœŸå®æ ·æœ¬ç¼“å­˜ =====
    if cfg['shots'] == 0:
        print("\nâš ï¸  æ£€æµ‹åˆ°0-shoté…ç½®ï¼Œå°†ä¸ä½¿ç”¨çœŸå®æ ·æœ¬ç¼“å­˜")
        print("   çœŸæ­£çš„0-shotåº”è¯¥å®Œå…¨ä¾èµ–äºï¼š")
        print("   1. CLIPé¢„è®­ç»ƒçŸ¥è¯†")
        print("   2. GPT-3æ–‡æœ¬æç¤ºè¯")
        print("   3. DALL-Eç”Ÿæˆçš„åˆæˆå›¾åƒ")
        print("   4. æµå½¢å­¦ä¹ å¢å¼ºçš„ç‰¹å¾")
        
        # è·å–ç±»åˆ«æ•°é‡
        num_classes = len(dataset.classnames)
        
        # åˆ›å»ºç©ºçš„ç¼“å­˜å¼ é‡
        # CLIP RN50ç‰¹å¾ç»´åº¦: 1024, DINO ResNet50ç‰¹å¾ç»´åº¦: 2048
        clip_cache_keys = torch.zeros(1024, 0, dtype=torch.float16).cuda()
        clip_cache_values = torch.zeros(0, num_classes, dtype=torch.float16).cuda()
        dino_cache_keys = torch.zeros(2048, 0, dtype=torch.float16).cuda()
        dino_cache_values = torch.zeros(0, num_classes, dtype=torch.float16).cuda()
        
        print(f"   åˆ›å»ºç©ºç¼“å­˜: CLIP keys {clip_cache_keys.shape}, values {clip_cache_values.shape}")
        print(f"              DINO keys {dino_cache_keys.shape}, values {dino_cache_values.shape}")
        
        # éªŒè¯ï¼šç¡®ä¿æ²¡æœ‰æ„å¤–åŠ è½½æ—§çš„0-shotç¼“å­˜æ–‡ä»¶
        zero_shot_files = [
            f"{cfg['cache_dir']}/clip_keys_0shots.pt",
            f"{cfg['cache_dir']}/clip_values_0shots.pt",
            f"{cfg['cache_dir']}/dino_keys_0shots.pt",
            f"{cfg['cache_dir']}/dino_values_0shots.pt"
        ]
        
        for file_path in zero_shot_files:
            if os.path.exists(file_path):
                print(f"\n   âš ï¸  è­¦å‘Š: å‘ç°æ—§çš„0-shotç¼“å­˜æ–‡ä»¶: {file_path}")
                print(f"      è¯¥æ–‡ä»¶å°†è¢«å¿½ç•¥ï¼Œå¦‚éœ€æ¸…ç†è¯·æ‰‹åŠ¨åˆ é™¤")
    
    else:
        # é0-shotï¼šæ­£å¸¸åŠ è½½çœŸå®æ ·æœ¬ç¼“å­˜
        print(f"\nåŠ è½½ {cfg['shots']}-shot çœŸå®æ ·æœ¬ç¼“å­˜...")
        print("\nConstructing CLIP cache model.")
        clip_cache_keys, clip_cache_values = build_clip_cache_model(cfg, clip_model, train_loader_cache)
        print("\nConstructing DINO cache model.")
        dino_cache_keys, dino_cache_values = build_dino_cache_model(cfg, dino_model, train_loader_cache)
        
        # éªŒè¯åŠ è½½çš„ç¼“å­˜å¤§å°æ˜¯å¦åˆç†
        expected_samples = cfg['shots'] * len(dataset.classnames)
        actual_clip_samples = clip_cache_keys.shape[1]
        actual_dino_samples = dino_cache_keys.shape[1]
        
        print(f"\nç¼“å­˜éªŒè¯:")
        print(f"  æœŸæœ›æ ·æœ¬æ•° (shots Ã— ç±»åˆ«æ•°): {cfg['shots']} Ã— {len(dataset.classnames)} = {expected_samples}")
        print(f"  å®é™…CLIPæ ·æœ¬æ•°: {actual_clip_samples}")
        print(f"  å®é™…DINOæ ·æœ¬æ•°: {actual_dino_samples}")


    print("\nConstructing cache model by dalle image.")
    print("\nConstructing CLIP cache model.")
    clip_dalle_cache_keys, clip_dalle_cache_values = build_clip_dalle_cache_model(cfg, clip_model, dalle_train_loader_cache)
    # ä¿å­˜CLIP DALLEç¼“å­˜æ¨¡å‹
    torch.save(clip_dalle_cache_keys, cfg['cache_dir'] + "/clip_dalle_keys_" + str(cfg['dalle_shots']) + "shots.pt")
    torch.save(clip_dalle_cache_values, cfg['cache_dir'] + "/clip_dalle_values_" + str(cfg['dalle_shots']) + "shots.pt")

    print("\nConstructing DINO cache model.")
    dino_dalle_cache_keys, dino_dalle_cache_values = build_dino_dalle_cache_model(cfg, dino_model, dalle_train_loader_cache)
    # ä¿å­˜DINO DALLEç¼“å­˜æ¨¡å‹
    torch.save(dino_dalle_cache_keys, cfg['cache_dir'] + "/dino_dalle_keys_" + str(cfg['dalle_shots']) + "shots.pt")
    torch.save(dino_dalle_cache_values, cfg['cache_dir'] + "/dino_dalle_values_" + str(cfg['dalle_shots']) + "shots.pt")

    # æ·»åŠ VAEç¼“å­˜æ¨¡å‹
    clip_vae_cache_keys = None
    clip_vae_cache_values = None
    dino_vae_cache_keys = None
    dino_vae_cache_values = None
    
    if use_vae and vae_train_loader_cache is not None:
        print("\nConstructing cache model by VAE generated image.")
        print("\nConstructing CLIP cache model.")
        clip_vae_cache_keys, clip_vae_cache_values = build_clip_vae_cache_model(cfg, clip_model, vae_train_loader_cache)
        # ä¿å­˜CLIP VAEç¼“å­˜æ¨¡å‹
        torch.save(clip_vae_cache_keys, cfg['cache_dir'] + "/clip_vae_keys_" + str(cfg['vae_shots']) + "shots.pt")
        torch.save(clip_vae_cache_values, cfg['cache_dir'] + "/clip_vae_values_" + str(cfg['vae_shots']) + "shots.pt")
        
        print("\nConstructing DINO cache model.")
        dino_vae_cache_keys, dino_vae_cache_values = build_dino_vae_cache_model(cfg, dino_model, vae_train_loader_cache)
        # ä¿å­˜DINO VAEç¼“å­˜æ¨¡å‹
        torch.save(dino_vae_cache_keys, cfg['cache_dir'] + "/dino_vae_keys_" + str(cfg['vae_shots']) + "shots.pt")
        torch.save(dino_vae_cache_values, cfg['cache_dir'] + "/dino_vae_values_" + str(cfg['vae_shots']) + "shots.pt")

    # Pre-load val features
    print("\nLoading CLIP feature from val set.")
    val_clip_features, val_labels = pre_CLIP_load_features(cfg, "val", clip_model, val_loader)
    print("\nLoading DINO feature from val set.")
    val_dino_features, val_labels = pre_DINO_load_features(cfg, "val", dino_model, val_loader)

    # Pre-load test features
    print("\nLoading CLIP feature from test set.")
    test_clip_features, test_labels = pre_CLIP_load_features(cfg, "test", clip_model, test_loader)
    print("\nLoading DINO feature from test set.")
    test_dino_features, test_labels = pre_DINO_load_features(cfg, "test", dino_model, test_loader)

    # ------------------------------------------ Tip-Adapter-F ------------------------------------------

    # åˆå¹¶æ‰€æœ‰ç¼“å­˜é”®å’Œå€¼
    all_clip_cache_keys = [clip_cache_keys, clip_dalle_cache_keys]
    all_clip_cache_values = [clip_cache_values, clip_dalle_cache_values]
    all_dino_cache_keys = [dino_cache_keys, dino_dalle_cache_keys]
    all_dino_cache_values = [dino_cache_values, dino_dalle_cache_values]
    
    # å¦‚æœä½¿ç”¨VAEï¼Œæ·»åŠ VAEç¼“å­˜
    if use_vae and clip_vae_cache_keys is not None:
        all_clip_cache_keys.append(clip_vae_cache_keys)
        all_clip_cache_values.append(clip_vae_cache_values)
        all_dino_cache_keys.append(dino_vae_cache_keys)
        all_dino_cache_values.append(dino_vae_cache_values)
    
    # åˆå¹¶æ‰€æœ‰ç¼“å­˜
    merged_clip_cache_keys = torch.cat(all_clip_cache_keys, dim=1)
    merged_clip_cache_values = torch.cat(all_clip_cache_values, dim=0)
    merged_dino_cache_keys = torch.cat(all_dino_cache_keys, dim=1)
    merged_dino_cache_values = torch.cat(all_dino_cache_values, dim=0)

    run_ensemble_tip_dalle_adapter_F(cfg, 
                            merged_clip_cache_keys, 
                            merged_clip_cache_values, 
                            val_clip_features,
                            test_clip_features, 
                            merged_dino_cache_keys, 
                            merged_dino_cache_values,
                            val_dino_features, 
                            test_dino_features, 
                            val_labels,
                            test_labels, 
                            clip_weights, 
                            clip_model, 
                            dino_model, 
                            train_loader_F,
                            dalle_train_loader_F,
                            vae_train_loader_F)
                            
if __name__ == '__main__':
    main()